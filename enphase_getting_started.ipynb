{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EnPhase Getting Started\n",
    "- A starter notebook to sort out some of the basic syntax, data nodels, data content, etc. of EnPhase data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dtm\n",
    "import matplotlib.dates as mpd\n",
    "import pytz\n",
    "tzutc = pytz.timezone('UTC')\n",
    "import h5py\n",
    "#\n",
    "#import operator\n",
    "import math\n",
    "import random\n",
    "import numpy\n",
    "import scipy\n",
    "#import scipy.optimize as spo\n",
    "from scipy import interpolate\n",
    "import scipy.constants\n",
    "import itertools\n",
    "import sys\n",
    "#import scipy.optimize as spo\n",
    "import os\n",
    "#import operator\n",
    "#from PIL import Image as ipp\n",
    "import multiprocessing as mpp\n",
    "#\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#import json\n",
    "#import pickle\n",
    "#\n",
    "import requests\n",
    "#\n",
    "import geopy.distance\n",
    "#from geopy.distance import vincenty\n",
    "#from geopy.distance import great_circle\n",
    "#\n",
    "#import shapely.geometry as sgp\n",
    "os.environ['PROJ_LIB'] = '{}/anaconda3/share/proj'.format(os.getenv('HOME'))\n",
    "#\n",
    "from mpl_toolkits.basemap import Basemap as Basemap\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from geographiclib.geodesic import Geodesic as ggp\n",
    "#\n",
    "#\n",
    "import random\n",
    "import geopy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enphase_data:  {'api_key': '9df0b8fc269146793c862d8e241aa220'}\n"
     ]
    }
   ],
   "source": [
    "enphase_data_file='enphase_ids.csv'\n",
    "enphase_data={}\n",
    "with open(enphase_data_file, 'r') as fin:\n",
    "    for rw in fin:\n",
    "        if rw.startswith('#'):\n",
    "            continue\n",
    "        #\n",
    "        ky, vl = rw.replace('\\n', '').split('\\t')\n",
    "        enphase_data[ky] = vl\n",
    "    #\n",
    "#\n",
    "print('enphase_data: ', enphase_data)\n",
    "#\n",
    "api_key = enphase_data['api_key']\n",
    "#\n",
    "# I don't actually understand what the user_id is. this is a demo-user id...\n",
    "enphase_user_id = '4d7a45774e6a41320a'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First demo query:\n",
    "\n",
    "### Summary:\n",
    "- I confess that at this time, I don't quite get the user_id parameter. I don't see a user_id code for my ID; my ID does not work, but I can use the demo-user and my api-key.\n",
    "- This query will give a \"summary\" record for system_id=67 (unless we change that variable later)\n",
    "- Note that we're being a little sloppy about the query string; the more proper way to do this is probably to use some sort of \"encode()\" function to build the QS, otherwise, we can make a lot of silly mistakes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** url:  https://api.enphaseenergy.com/api/v2/systems/67/summary?key=9df0b8fc269146793c862d8e241aa220&user_id=4d7a45774e6a41320a\n",
      "** response:  <Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# fetch some data:\n",
    "# TODO: might need to use \"encode\" and other functions to build url and/or query string.\n",
    "#. but also, be careful, because there are multiple standards for q-string delimitation.\n",
    "#. i think i recently ran into this with the USGS ComCat API.\n",
    "#\n",
    "sys_id=67\n",
    "dataset='summary'\n",
    "#response = requests.get('https://api.enphaseenergy.com/api/v2/systems/67/summary?key={}'.format(enphase_data['api_key']))\n",
    "url = 'https://api.enphaseenergy.com/api/v2/systems/{}/{}?key={}&user_id={}'.format(\n",
    "    sys_id, dataset, enphase_data['api_key'], enphase_user_id)\n",
    "print('** url: ', url)\n",
    "\n",
    "response = requests.get(url)\n",
    "#\n",
    "print('** response: ', response)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** response return code:  200\n",
      "***\n",
      " {\"system_id\":67,\"modules\":31,\"size_w\":7690,\"current_power\":24,\"energy_today\":90,\"energy_lifetime\":112069333,\"summary_date\":\"2019-12-04\",\"source\":\"microinverters\",\"status\":\"normal\",\"operational_at\":1201362300,\"last_report_at\":1575479729,\"last_interval_end_at\":1575478848}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "print('*** response return code: ', response.status_code)\n",
    "\n",
    "data = response.content.decode()\n",
    "print('***\\n', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** code:  200\n",
      "** data:  {\"system_id\":67,\"modules\":31,\"size_w\":7690,\"current_power\":24,\"energy_today\":90,\"energy_lifetime\":112069333,\"summary_date\":\"2019-12-04\",\"source\":\"microinverters\",\"status\":\"normal\",\"operational_at\":1201362300,\"last_report_at\":1575479729,\"last_interval_end_at\":1575478848}\n"
     ]
    }
   ],
   "source": [
    "# make the request by passing parameters for the q-string.\n",
    "#\n",
    "core_req_prams = {'key': api_key, 'user_id': enphase_user_id}\n",
    "url2 = 'https://api.enphaseenergy.com/api/v2/systems/{}/{}'.format(sys_id, dataset)\n",
    "#\n",
    "response2 = requests.get(url2, params=core_req_prams,)\n",
    "#\n",
    "print('** code: ', response2.status_code)\n",
    "data = response.content.decode()\n",
    "#\n",
    "print('** data: ', data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Query\n",
    "- This is really our starting place (except that the first decent demo query we found (above) was for 'summary'\n",
    "- Do some index queries. Note, this will return multiple results. We can filter if we want\n",
    "- We will demo how to do multiple queries to select $N>N_{max}$. This is refered to under the scope of \"pagination\" in the documentation.\n",
    "- Basically, we'll do a query; note the ID of the final entry, and use that as the start point for the next query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next ID:  4e4441344d7a41340a\n",
      "len(systems):  1000\n"
     ]
    }
   ],
   "source": [
    "url_index = 'https://api.enphaseenergy.com/api/v2/systems'\n",
    "#\n",
    "# some prams are {next, limit, status, or status[] for multiple status values}\n",
    "index_prams = {'limit':1000}\n",
    "index_prams.update(core_req_prams)\n",
    "#\n",
    "response_index = requests.get(url_index, params=index_prams)\n",
    "#\n",
    "print('*** code: ', response_index.status_code)\n",
    "#\n",
    "\n",
    "#data_b = response_index.content\n",
    "\n",
    "data_json = json.loads(response_index.content.decode())\n",
    "\n",
    "print('keys: ', data_json.keys()\n",
    "\n",
    "print('next ID: ', data_json['next'])\n",
    "print('len(systems): ', len(data_json['systems']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** n_key_errors:  390\n",
      "** unique keys:  ['system_id', 'system_name', 'system_public_name', 'status', 'timezone', 'country', 'state', 'city', 'postal_code', 'connection_type', 'meta', 'other_references']\n"
     ]
    }
   ],
   "source": [
    "# This generally works, but not all entries have identical keys. It looks like some, but not all, entries\n",
    "#. have a key:val called \"other_references\"\n",
    "#\n",
    "#\n",
    "kys0 = data_json['systems'][0].keys()\n",
    "n_errors = 0\n",
    "unique_keys = []\n",
    "for k,rw in enumerate(data_json['systems'][1:]):\n",
    "    #print('** rw[{}]: {}'.format(k,rw) )\n",
    "    #\n",
    "    for ky in rw.keys():\n",
    "        if not ky in unique_keys:\n",
    "            unique_keys += [ky]\n",
    "    #\n",
    "    #\n",
    "    kys = rw.keys()\n",
    "    if not list(kys)==list(kys0):\n",
    "        #print('***  error: keys[{},{}] do not match!!!'.format(k, k+1))\n",
    "        #print('* * ', kys0)\n",
    "        #print('* * ', kys)\n",
    "        n_errors += 1\n",
    "    kys0=kys\n",
    "    #\n",
    "    #if k>15: break\n",
    "#\n",
    "print('** n_key_errors: ', n_errors)\n",
    "print('** unique keys: ', unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***  {'system_id': 67, 'system_name': 'Eich Residence', 'system_public_name': 'Eich Residence', 'status': 'normal', 'timezone': 'America/Los_Angeles', 'country': 'US', 'state': 'CA', 'city': 'Sebastopol', 'postal_code': '95472', 'other_references': ['Solarfox'], 'connection_type': 'wifi', 'meta': {'status': 'normal', 'last_report_at': 1575481783, 'last_energy_at': 1575481069, 'operational_at': 1201362300}}\n"
     ]
    }
   ],
   "source": [
    "print('*** ', data_json['systems'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something more operational...\n",
    "1. Determine which column data we want to collect and handle cases where those data do not exist\n",
    "2. Note that we basically have to decide which columns to use up front. We can write scripts to do this dynamically, but in practice we'll usually spin through the data to determine/guess a unique and/or desired set of columnns\n",
    "2. How do we concatenate with multiple queries?\n",
    "\n",
    "This nested-dict type data structure is really nice for row-by-row work, and being totally unstructured can make it easy to produce and maintain, but a bit messy and slow to work with. But nothing we can't handle.\n",
    "\n",
    "Let's get 2000 records.\n",
    "\n",
    "### What we find:\n",
    "- We correctly show how to do an index query 1000 records (or any n) at a time\n",
    "- We (almost) correctly tansform the JSON dict-like structure to a structured list\n",
    "- We discover that the 'meta' column is a nested dict-like structure, so we'll need to look at that more closely. What questions do we want to ask? Do we need to expand those out?\n",
    "- Operatioally, do we need to just pull this all back to a similar data structure?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query steps:\n",
      "n_records: 2000, n_batch: 1000, n_steps: 2\n",
      "** qs_prams:  {'limit': 1000, 'key': '9df0b8fc269146793c862d8e241aa220', 'user_id': '4d7a45774e6a41320a'}\n",
      "*** code:  200\n",
      "keys:  dict_keys(['systems', 'next'])\n",
      "next ID:  4e4441344d7a41340a\n",
      "len(systems):  1000\n",
      "** qs_prams:  {'limit': 1000, 'key': '9df0b8fc269146793c862d8e241aa220', 'user_id': '4d7a45774e6a41320a', 'next': '4e4441344d7a41340a'}\n",
      "*** code:  200\n",
      "keys:  dict_keys(['systems', 'next'])\n",
      "next ID:  4e6a4d344f5459790a\n",
      "len(systems):  1000\n"
     ]
    }
   ],
   "source": [
    "n_records = 2000\n",
    "n_batch=1000\n",
    "n_steps = int(numpy.ceil(n_records/n_batch))\n",
    "#\n",
    "print('query steps:\\nn_records: {}, n_batch: {}, n_steps: {}'.format(n_records, n_batch, n_steps))\n",
    "#\n",
    "# we can use any number of data containers. We should probably use a rec-array, though most people would\n",
    "#. likley opt for a PANDAS data frame. right now, I don't want to sort out the string-lengths for rec-array\n",
    "#. and I hate PANDAS (though I confess it is probably a good choice to get started here). For nowk\n",
    "#. let's just make a cols-header and lump the data into a list-type. Note that we can pre-construct\n",
    "#\n",
    "delim = chr(9)    # tab\n",
    "cols = unique_keys\n",
    "data = []\n",
    "index_prams = {'limit':1000}\n",
    "index_prams.update(core_req_prams)\n",
    "next_key=''\n",
    "#\n",
    "for step in range(n_steps):\n",
    "    #\n",
    "    my_prams = index_prams.copy()\n",
    "    if not next_key=='':\n",
    "        my_prams['next']=next_key\n",
    "    print('** qs_prams: ', my_prams)\n",
    "    #\n",
    "    response_index = requests.get(url_index, params=my_prams)\n",
    "    #\n",
    "    print('*** code: ', response_index.status_code)\n",
    "    #\n",
    "    # TODO: in real life, we'd have some error handling here. we would evaluate the response_code\n",
    "    #. to determine whether or not the query was successful.\n",
    "    #\n",
    "    data_json = json.loads(response_index.content.decode())\n",
    "    next_key = data_json['next']\n",
    "    #\n",
    "    # now, spin through the systems:\n",
    "    for sys_rw in data_json['systems']:\n",
    "        #\n",
    "        # could also do this with by fetching key,vals and sorting by cols or something, but\n",
    "        #. this is pretty bullet-proof:\n",
    "        data += [[sys_rw.get(ky, None) for ky in cols]]\n",
    "    #\n",
    "    print('keys: ', data_json.keys())\n",
    "    print('next ID: ', data_json['next'])\n",
    "    print('len(systems): ', len(data_json['systems']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data):  2000\n"
     ]
    }
   ],
   "source": [
    "# let's have a quick look at the data:\n",
    "#\n",
    "print('len(data): ', len(data))\n",
    "#\n",
    "\n",
    "print('cols: {}\\n'.format(cols))\n",
    "for rw in data[0:10]:\n",
    "    print('** ', rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
